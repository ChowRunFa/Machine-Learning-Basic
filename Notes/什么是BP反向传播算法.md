## 什么是BP反向传播算法

神经网络是个好工具，但就像有的刀削铁如泥，有的却只能拿来切豆腐。

真正决定神经网络好不好用的是神经元之间连接的权重和神经元的阈值。

<img src="https://img-blog.csdnimg.cn/3fa5a25d13c54501a7a8faec93282bc6.png" alt="请添加图片描述" style="zoom:40%;" />

如何确定这些数字，大部分时间我们都在使用反向传播，也就是常说的BP(Back Propagation)算法。

<img src="https://img-blog.csdnimg.cn/d101c7c81d554253978af5aa2e10c8a2.png" alt="请添加图片描述" style="zoom:33%;" />

BP算法的思想非常简单，即根据网络输出的答案与正确答案之间的误差，不断调整网络的参数。

假设我们正在训练一个图片分类网络，输入一张图片逐层向前计算后，网络会给出它属于某一类事物的概率，由于每个神经网络的初始参数是随机赋予的，大部分时间答案都不尽如人意。

这时我们可以根据网络输出与正确答案之间的差距，从最后一层开始逐层向前开始调整神经网络的参数。如果误差值为负，我们就提升权重，反之就降低权重。

<img src="https://img-blog.csdnimg.cn/20130a8454c84a029e2f5014dec0e179.png" alt="请添加图片描述" style="zoom:50%;" />

调整的程度受一定的比率即“学习率”的制约，它像一个旋钮，用来控制参数调整程度的高低。在一次次输入数据和反向调整中，网络就能逐渐给出不错的输出。

由于强大的调整能力，BP算法控制下的神经网络很容易**过拟合**，也就是在训练数据上表现的很好，却认不出新数据是什么。这时我们可以采用**“提前停止”**策略，也就是将数据按一定比例划分为**“训练集”和“验证集”**，用训练集调整参数，用验证集估算误差，如果训练集误差降低的同时验证集的误差在升高，就代表网络开始过于“适应”训练集，这时就可以结束训练。

